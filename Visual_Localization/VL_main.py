# Copyright 2022 the Regents of the University of California, Nerfstudio Team and contributors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# !/usr/bin/env python
"""
render.py
"""
from __future__ import annotations

import cv2
import numpy as np
from skimage import io
import random
import time
import imageio
from tqdm import *
import shutil

# -- for VL task
from Data_loader.loader_utils import *
from geometry_utils import *
from Matchers.LoFTR.LoFTR_api import LoFTR_Matcher, draw_matches
from Matchers.SuperGlue.SuperGlue_api import SuperGlue_Matcher
from Matchers.LightGlue.LightGlue_api import LightGlue_Matcher

# -- change the working directory
import os
current_dir = os.getcwd()
new_dir = os.path.abspath(os.path.join(current_dir, ".."))
os.chdir(new_dir)

# --
to8b = lambda x: (255 * np.clip(x, 0, 1)).astype(np.uint8)

# --
random.seed(0)
np.random.seed(0)

#--
if_record_init_error = False #-- for convenience

@dataclass
class CropData:
    """Data for cropping an image."""

    background_color: Float[Tensor, "3"] = torch.Tensor([0.0, 0.0, 0.0])
    """background color"""
    center: Float[Tensor, "3"] = torch.Tensor([0.0, 0.0, 0.0])
    """center of the crop"""
    scale: Float[Tensor, "3"] = torch.Tensor([2.0, 2.0, 2.0])
    """scale of the crop"""


def get_crop_from_json(camera_json: Dict[str, Any]) -> Optional[CropData]:
    """Load crop data from a camera path JSON

    args:
        camera_json: camera path data
    returns:
        Crop data
    """
    if "crop" not in camera_json or camera_json["crop"] is None:
        return None

    bg_color = camera_json["crop"]["crop_bg_color"]

    return CropData(
        background_color=torch.Tensor([bg_color["r"] / 255.0, bg_color["g"] / 255.0, bg_color["b"] / 255.0]),
        center=torch.Tensor(camera_json["crop"]["crop_center"]),
        scale=torch.Tensor(camera_json["crop"]["crop_scale"]),
    )


@dataclass
class BaseRender:
    """Base class for rendering."""

    load_config: Path
    """Path to config YAML file."""
    output_path: Path = Path("renders/output.mp4")
    """Path to output video file."""
    image_format: Literal["jpeg", "png"] = "jpeg"
    """Image format"""
    jpeg_quality: int = 100
    """JPEG quality"""
    downscale_factor: float = 1.0
    """Scaling factor to apply to the camera image resolution."""
    eval_num_rays_per_chunk: Optional[int] = None
    """Specifies number of rays per chunk during eval. If None, use the value in the config file."""
    colormap_options: colormaps.ColormapOptions = colormaps.ColormapOptions()
    """Colormap options."""
    #--
    dataset_path: str = r"data/scenes/"
    """data for visual localization"""
    scene_name: str = None
    """scene name"""
    sequence_id: str = None
    """seq id"""
    MATCH_ALGORITHM: str = 'LoFTR'  # -- 'LoFTR' or 'SuperGlue' or 'LightGlue'
    """matching algorithm"""
    NERF_ALGORITHM: str = 'Nerfacto' # -- 'Nerfacto' or 'Instant-NGP'
    """nerf algorithm"""

@dataclass
class RenderCameraPath(BaseRender):
    """Render a camera path generated by the viewer or blender add-on."""

    rendered_output_names: List[str] = field(default_factory=lambda: ["rgb"])
    """Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis"""
    camera_path_filename: Path = Path("camera_path.json")
    """Filename of the camera path to render."""
    output_format: Literal["images", "video"] = "video"
    """How to save output data."""

    def main(self) -> None:
        """ record processing time """
        start_time = time.time()

        """ Main function."""
        _, self.pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,
            test_mode="inference",
        )
        install_checks.check_ffmpeg_installed()
        self.output_format = "images"

        """ load cameras for visual localization... """
        # -- target cameras
        camera_file_path = f"{self.dataset_path}/{self.scene_name}/seq_{self.sequence_id}/{self.scene_name}-seq-{self.sequence_id}/transforms.json"
        scene_name = camera_file_path.split("/")[-2]
        transform_file = str(Path(self.load_config.parent, "dataparser_transforms.json"))
        self.camera_path, self.obs_images, self.obs_images_ir, self.nerf2metashape = load_poses_from_json(camera_file_path, transform_file,
                                                                                     downscale_factor=4)
        camera_num = len(self.obs_images_ir)

        """ load scale parameters """
        scale_file_path = f"{self.dataset_path}/{self.scene_name}/scale.json"
        meta = load_from_json(scale_file_path)
        self.metashape2world = meta["metashape2world"]

        """ output file path """
        self.output_file_path = self.output_path / self.scene_name / self.NERF_ALGORITHM /self.MATCH_ALGORITHM

        """ start VL loop """
        print(f"\nScene is {self.scene_name}")
        print(f"Seq id is {self.sequence_id}")
        print(f"NeRF model is {self.NERF_ALGORITHM}")
        print(f"Matching algorithm is {self.MATCH_ALGORITHM}\n")

        VL_logs = []
        for i in trange(camera_num):
            camera_idx = i

            if i < camera_num - 6:
                camera_idx_gt = i + 5
            else:
                camera_idx_gt = i - 5

            # camera_idx_gt = camera_idx #-- 初始位姿的误差直接手动给偏移
            cur_log = self.visual_localization_func(i, camera_idx, camera_idx_gt, self.nerf2metashape,
                                                    self.metashape2world, max_iter_num=3)
            if cur_log is not None:
                VL_logs.append(cur_log)

        """ save VL results """
        if not os.path.exists(self.output_file_path):
            os.makedirs(self.output_file_path)

        with open(f"{self.output_file_path}/results_{scene_name}.txt", 'w') as f:
            f.write(f"------------------ Visual Localization Results error ------------------\n")
            # f.write(f"Phi_err(degree), Omega_err(degree), Kappa_err(degree), X_err(m), Y_err(m), Z_err(m)\n")
            f.write(f"Angel Errors(degree)   Translation Errors(meter)  Time per image(second)\n")
            idx = 1
            for log in VL_logs:
                # cur_err = log["final_error"]
                cur_time = log["cost_time"]
                angel_err = log["angel_err"]
                trans_err = log["translation_err"]
                # f.write(f"image_idx:{idx}: {cur_err[0]}  {cur_err[1]}  {cur_err[2]}  {cur_err[3]}  {cur_err[4]}  {cur_err[5]}  angel_err:{angel_err}  translation_err:{trans_err}  time:{cur_time}\n")
                f.write(
                    f"image_idx:{idx}:  angel_err:{angel_err}  translation_err:{trans_err}  time:{cur_time}\n")

                idx += 1

        print("out of visual localization function, OK")

    def visual_localization_func(self, iter, camera_idx, camera_idx_gt, nerf2metashape_scale,
                                 metashape2world_scale, max_iter_num):
        """

        Args:
            iter:
            camera_idx:
            camera_idx_gt:
            nerf2metashape_scale:
            metashape2world_scale:
            max_iter_num:

        Returns:

        """
        save_images = False

        """ load GT pose c2w to check """
        c2w_gt = self.camera_path.camera_to_worlds[camera_idx_gt].detach().cpu().numpy()

        """ initial pose c2w """
        if 1:
            c2w = self.camera_path.camera_to_worlds[camera_idx].detach().cpu().numpy()
        else:
            c2w = self.camera_path.camera_to_worlds[camera_idx].detach().cpu().numpy()
            c2w_4x4 = np.vstack((c2w, np.array([0, 0, 0, 1])))

            # -- set initial pose error
            err_rotation = eulerAnglesToRotationMatrix(10, 10, 0)
            err_translation = np.array([-0.1, 0.2, 0.1]).reshape(3, 1)

            err_trans = np.hstack((err_rotation, err_translation))
            err_trans = np.vstack((err_trans, np.array([0, 0, 0, 1])))

            c2w_4x4 = err_trans @ c2w_4x4
            c2w = c2w_4x4[:3, :]
            self.camera_path.camera_to_worlds[camera_idx] = torch.tensor(c2w)

        """ output logs """
        output_logs = {
            "initial_error": [],
            "final_error": [],
            "cost_time": float(0),
            "angel_err": float(0),
            "translation_err": float(0),
        }

        """ initial pose error """
        phi_err, omega_err, kappa_err, x_err, y_err, z_err = getAngleTranslationErr(c2w, c2w_gt)
        x_err, y_err, z_err = x_err * (nerf2metashape_scale * metashape2world_scale), \
                              y_err * (nerf2metashape_scale * metashape2world_scale), \
                              z_err * (nerf2metashape_scale * metashape2world_scale)

        output_logs["initial_error"] = [phi_err, omega_err, kappa_err, x_err, y_err, z_err]

        if if_record_init_error:
            #-- record the init pose error.
            output_logs["angel_err"] = phi_err + omega_err + kappa_err
            output_logs["translation_err"] = (x_err ** 2 + y_err ** 2 + z_err ** 2) ** 0.5
            return output_logs

        if 1:
            """ check supervised cameras poses. """
            camera_num = len(os.listdir(
                r"/media/xiapanwang/主数据盘/xiapanwang/Codes/python/NeRFs/NS/NS/nerfstudio/data/scenes/facility-room/seq_3/images"))
            if os.path.exists(f"{self.output_file_path}/check_cameras"):
                shutil.rmtree(f"{self.output_file_path}/check_cameras")
                os.makedirs(f"{self.output_file_path}/check_cameras")
            else:
                os.makedirs(f"{self.output_file_path}/check_cameras")

            for i in trange(camera_num):
                obs_image = self.obs_images[i]

                render_rgb, render_depth = render_current_pose(
                    self.pipeline,
                    self.camera_path,
                    i,
                    output_filename=self.output_path)

                io.imsave(f"{self.output_file_path}/check_cameras/check_{i + 1}_r.png", render_rgb)
                io.imsave(f"{self.output_file_path}/check_cameras/check_{i + 1}_o.png", obs_image)
                # io.imsave(f"{self.output_file_path}/check_cameras/check_{i + 1}_d.png", render_depth)

                # cv2.imwrite(f"{self.output_file_path}/check_cameras/check_{i + 1}_r.png", render_rgb)
                # cv2.imwrite(f"{self.output_file_path}/check_cameras/check_{i + 1}_o.png", obs_image)
                cv2.imwrite(f"{self.output_file_path}/check_cameras/check_{i + 1}_d.png", render_depth*(255/np.max(render_depth)))



            return 1


        """ Start VL iterations """
        cut_time_1 = time.time()

        for k in range(max_iter_num + 1):

            if k > max_iter_num - 1:
                break
            # print(f"Iteration {k + 1}/{max_iter_num}")

            """ 1. get query image """
            # obs_image = self.obs_images[camera_idx_gt]  # -- numpy
            obs_image = self.obs_images_ir[camera_idx_gt]
            if len(obs_image.shape) < 3:
                if obs_image[0][0] == 9999:
                    return None

            """ 2. get render image """
            render_rgb, render_depth = render_current_pose(
                self.pipeline,
                self.camera_path,
                camera_idx,
                output_filename=self.output_file_path)
            # -- visualize
            if save_images:  # -- for checking
                render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_{k}_render.png", render_rgb_out)
                cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_{k}_obs.png", obs_image)


            """ 3. Matching """
            render_rgb = (render_rgb / 255).astype('float32')
            obs_image = (obs_image / 255).astype('float32')

            # -- 2d pts matching
            # -- mkpts0中的存储顺序：[index_of_W, index_of_H]，x向右，y向下
            # -- 解释这四个参数：(1) mkpts0,1: matched key points of image 0,1; (2)c/mconf: coarse/... matrix of confidence
            if self.MATCH_ALGORITHM == 'LoFTR':
                try:
                    mkpts0, mkpts1, mconf, _ = LoFTR_Matcher(render_rgb, obs_image)
                except:
                    output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                    output_logs["cost_time"] = 99999
                    output_logs["angel_err"] = 99999
                    output_logs["translation_err"] = 99999

                    if save_images:
                        render_rgb = (render_rgb * 255)
                        obs_image = (obs_image * 255)

                        render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

            elif self.MATCH_ALGORITHM == 'SuperGlue':
                try:
                    mkpts0, mkpts1, mconf = SuperGlue_Matcher(render_rgb, obs_image)
                except:
                    output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                    output_logs["cost_time"] = 99999
                    output_logs["angel_err"] = 99999
                    output_logs["translation_err"] = 99999

                    if save_images:
                        render_rgb = (render_rgb * 255)
                        obs_image = (obs_image * 255)

                        render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

            elif self.MATCH_ALGORITHM == 'LightGlue':
                # -- Lightglue appears to be incompatible with grayscale images, so we stack them into 3 channels.
                obs_image_3_channels = cv2.cvtColor(obs_image[:, :, np.newaxis], cv2.COLOR_GRAY2RGB)
                try:
                    # -- mode, 0:superpoint+lightglue, 1: disk+lightglue
                    mkpts0, mkpts1 = LightGlue_Matcher(render_rgb, obs_image_3_channels, mode=0)
                except:
                    output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                    output_logs["cost_time"] = 99999
                    output_logs["angel_err"] = 99999
                    output_logs["translation_err"] = 99999

                    if save_images:
                        render_rgb = (render_rgb * 255)
                        obs_image = (obs_image * 255)

                        render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                        cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

            else:
                # -- 找个时间把这里改写成警报
                return -1

            # -- 用一个简单的粗差滤除的方法，经过测试，必须要用粗差剔除，不然会导致定位失败
            if len(mkpts0) < 5:
                output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                output_logs["cost_time"] = 99999
                output_logs["angel_err"] = 99999
                output_logs["translation_err"] = 99999

                if save_images:
                    render_rgb = (render_rgb * 255)
                    obs_image = (obs_image * 255)

                    render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

                return output_logs

            mkpts0_new, mkpts1_new = filter_matches_using_homography(mkpts0, mkpts1)

            if len(mkpts0) < 5:
                output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                output_logs["cost_time"] = 99999
                output_logs["angel_err"] = 99999
                output_logs["translation_err"] = 99999

                if save_images:
                    render_rgb = (render_rgb * 255)
                    obs_image = (obs_image * 255)

                    render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

                return output_logs

            selected_num = 200
            if len(mkpts0_new) >= selected_num:
                sample_pts = random.sample([i for i in range(len(mkpts0_new))], selected_num)
            else:
                sample_pts = [i for i in range(len(mkpts0_new))]

            mkpts0_new, mkpts1_new = mkpts0_new[sample_pts], mkpts1_new[sample_pts]

            if len(mkpts0_new) < 5:
                output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                output_logs["cost_time"] = 99999
                output_logs["angel_err"] = 99999
                output_logs["translation_err"] = 99999

                if save_images:
                    render_rgb = (render_rgb * 255)
                    obs_image = (obs_image * 255)

                    render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

                return output_logs

            mkpts0_new, mkpts1_new = filter_matches_using_homography(mkpts0_new, mkpts1_new)

            if len(mkpts0_new) < 5:
                output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
                output_logs["cost_time"] = 99999
                output_logs["angel_err"] = 99999
                output_logs["translation_err"] = 99999

                if save_images:
                    render_rgb = (render_rgb * 255)
                    obs_image = (obs_image * 255)

                    render_rgb_out = cv2.cvtColor(render_rgb, cv2.COLOR_RGB2BGR)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_render.png", render_rgb_out)
                    cv2.imwrite(f"{self.output_file_path}/seq_{self.sequence_id}_{iter}_obs.png", obs_image)

                return output_logs

            # -- visualize final matching results
            # draw_matches(f"{self.output_file_path}", render_rgb, obs_image, mkpts0_new, mkpts1_new, k)

            """ 4. convert 2d pts to 3d pts """
            sub_rays_o, sub_rays_d = get_rays_subpixel_np(mkpts0_new, self.camera_path, camera_idx)
            sub_depths = get_rays_depth_np(mkpts0_new, render_depth)
            pts_3d = generating_3d_pts(sub_rays_o, sub_rays_d,
                                       sub_depths)

            """ 5. solve PnP problem and update query image pose """
            fx = self.camera_path.fx[camera_idx][0].detach().cpu().numpy()
            fy = self.camera_path.fy[camera_idx][0].detach().cpu().numpy()
            cx = self.camera_path.cx[camera_idx][0].detach().cpu().numpy()
            cy = self.camera_path.cy[camera_idx][0].detach().cpu().numpy()

            # -- opencv takes k1,k2,p1,p2,k3, while distortion_params contains k1,k2,k3,k4,p1,p2
            distortion_param = self.camera_path.distortion_params[camera_idx].detach().cpu().numpy()[[0, 1, 4, 5, 2]]
            cam_intrinsic = np.array([[fx, 0, cx],
                                      [0, fy, cy],
                                      [0, 0, 1]])

            isSuccess, R, t, _ = cv2.solvePnPRansac(pts_3d, mkpts1_new, cam_intrinsic,
                                                    distortion_param, flags=0)  # -- 这里的R, t是w2c
            R_mid = cv2.Rodrigues(R)[0]
            w2c = np.vstack((np.hstack((R_mid, t.reshape(3, 1))), np.array([0, 0, 0, 1])))
            c2w = np.linalg.inv(w2c)[:3, :]
            c2w[:3, 1], c2w[:3, 2] = c2w[:3, 1] * (-1), c2w[:3, 2] * (-1)

            self.camera_path.camera_to_worlds[camera_idx] = torch.tensor(c2w)

        """ 6. calculate angle and translation errors"""
        phi_err, omega_err, kappa_err, x_err, y_err, z_err = 99999, 99999, 99999, 99999, 99999, 99999
        phi_err, omega_err, kappa_err, x_err, y_err, z_err = getAngleTranslationErr(c2w, c2w_gt)

        # -- rescale the model
        x_err, y_err, z_err = x_err * (nerf2metashape_scale * metashape2world_scale), \
                              y_err * (nerf2metashape_scale * metashape2world_scale), \
                              z_err * (nerf2metashape_scale * metashape2world_scale)

        # -- failure detection
        if (phi_err + omega_err + kappa_err) > 5 or ((x_err ** 2 + y_err ** 2 + z_err ** 2) ** 0.5) > 0.1:
            output_logs["final_error"] = [99999, 99999, 99999, 99999, 99999, 99999]
            output_logs["cost_time"] = 99999
            output_logs["angel_err"] = 99999
            output_logs["translation_err"] = 99999
            return output_logs

        # print(f"\nError in iteration {k + 1}/{max_iter_num}:\n"
        #       f"phi:{phi_err}°, omega:{omega_err}°, kappa:{kappa_err}°\n"
        #       f"angel error: {abs(phi_err) + abs(omega_err) + abs(kappa_err)}°\n"
        #       f"x:{x_err}m, y:{y_err}m, z:{z_err}m\n"
        #       f"translation error: {(x_err ** 2 + y_err ** 2 + z_err ** 2) ** 0.5}m")

        output_logs["final_error"] = [phi_err, omega_err, kappa_err, x_err, y_err, z_err]

        final_time = time.time()
        # print(f"Final time cost {final_time - cut_time_1}s")
        output_logs["cost_time"] = final_time - cut_time_1
        output_logs["angel_err"] = phi_err + omega_err + kappa_err
        output_logs["translation_err"] = (x_err ** 2 + y_err ** 2 + z_err ** 2) ** 0.5

        return output_logs


@dataclass
class RenderInterpolated(BaseRender):
    """Render a trajectory that interpolates between training or eval dataset images."""

    rendered_output_names: List[str] = field(default_factory=lambda: ["rgb"])
    """Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis"""
    pose_source: Literal["eval", "train"] = "eval"
    """Pose source to render."""
    interpolation_steps: int = 10
    """Number of interpolation steps between eval dataset cameras."""
    order_poses: bool = False
    """Whether to order camera poses by proximity."""
    frame_rate: int = 24
    """Frame rate of the output video."""
    output_format: Literal["images", "video"] = "video"
    """How to save output data."""


@dataclass
class SpiralRender(BaseRender):
    """Render a spiral trajectory (often not great)."""

    rendered_output_names: List[str] = field(default_factory=lambda: ["rgb"])
    """Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis"""
    seconds: float = 3.0
    """How long the video should be."""
    output_format: Literal["images", "video"] = "video"
    """How to save output data."""
    frame_rate: int = 24
    """Frame rate of the output video (only for interpolate trajectory)."""
    radius: float = 0.1
    """Radius of the spiral."""


Commands = tyro.conf.FlagConversionOff[
    Union[
        Annotated[RenderCameraPath, tyro.conf.subcommand(name="camera-path")],
        Annotated[RenderInterpolated, tyro.conf.subcommand(name="interpolate")],
        Annotated[SpiralRender, tyro.conf.subcommand(name="spiral")],
    ]
]


def entrypoint():
    """Entrypoint for use with pyproject scripts."""
    tyro.extras.set_accent_color("bright_yellow")
    tyro.cli(Commands).main()


if __name__ == "__main__":
    entrypoint()


def get_parser_fn():
    """Get the parser function for the sphinx docs."""
    return tyro.extras.get_parser(Commands)  # noqa
